{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4sv+Q/Eqe/BeL8aSE8QKi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghazouanihatim/DS2025/blob/main/CCFin/Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZelkfEc7Ye",
        "outputId": "23097ec1-9f3b-4607-a608-8e7bcdb86938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "credit_approval = fetch_ucirepo(id=27)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = credit_approval.data.features\n",
        "y = credit_approval.data.targets\n",
        "\n",
        "# metadata\n",
        "print(credit_approval.metadata)\n",
        "\n",
        "# variable information\n",
        "print(credit_approval.variables)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "342d7MFEk3se",
        "outputId": "cc0c12b4-15a6-4448-ad4e-402109ee0e73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 27, 'name': 'Credit Approval', 'repository_url': 'https://archive.ics.uci.edu/dataset/27/credit+approval', 'data_url': 'https://archive.ics.uci.edu/static/public/27/data.csv', 'abstract': 'This data concerns credit card applications; good mix of attributes', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 690, 'num_features': 15, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': [], 'target_col': ['A16'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1987, 'last_updated': 'Wed Aug 23 2023', 'dataset_doi': '10.24432/C5FS30', 'creators': ['J. R. Quinlan'], 'intro_paper': None, 'additional_info': {'summary': 'This file concerns credit card applications.  All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.\\r\\n  \\r\\nThis dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values.  There are also a few missing values.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'A1:\\tb, a.\\r\\nA2:\\tcontinuous.\\r\\nA3:\\tcontinuous.\\r\\nA4:\\tu, y, l, t.\\r\\nA5:\\tg, p, gg.\\r\\nA6:\\tc, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\\r\\nA7:\\tv, h, bb, j, n, z, dd, ff, o.\\r\\nA8:\\tcontinuous.\\r\\nA9:\\tt, f.\\r\\nA10:\\tt, f.\\r\\nA11:\\tcontinuous.\\r\\nA12:\\tt, f.\\r\\nA13:\\tg, p, s.\\r\\nA14:\\tcontinuous.\\r\\nA15:\\tcontinuous.\\r\\nA16: +,-         (class attribute)', 'citation': None}}\n",
            "   name     role         type demographic description units missing_values\n",
            "0   A16   Target  Categorical        None        None  None             no\n",
            "1   A15  Feature   Continuous        None        None  None             no\n",
            "2   A14  Feature   Continuous        None        None  None            yes\n",
            "3   A13  Feature  Categorical        None        None  None             no\n",
            "4   A12  Feature  Categorical        None        None  None             no\n",
            "5   A11  Feature   Continuous        None        None  None             no\n",
            "6   A10  Feature  Categorical        None        None  None             no\n",
            "7    A9  Feature  Categorical        None        None  None             no\n",
            "8    A8  Feature   Continuous        None        None  None             no\n",
            "9    A7  Feature  Categorical        None        None  None            yes\n",
            "10   A6  Feature  Categorical        None        None  None            yes\n",
            "11   A5  Feature  Categorical        None        None  None            yes\n",
            "12   A4  Feature  Categorical        None        None  None            yes\n",
            "13   A3  Feature   Continuous        None        None  None             no\n",
            "14   A2  Feature   Continuous        None        None  None            yes\n",
            "15   A1  Feature  Categorical        None        None  None            yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd              # Manipulation et analyse de données tabulaires\n",
        "import numpy as np               # Calculs numériques et opérations sur des arrays\n",
        "import sys                       # Importation du module sys pour les informations système\n",
        "import matplotlib                # Importation du module matplotlib entier\n",
        "\n",
        "# Importation des bibliothèques pour la visualisation\n",
        "import matplotlib.pyplot as plt  # Création de graphiques de base\n",
        "import seaborn as sns           # Visualisations statistiques avancées\n",
        "\n",
        "# Importation de bibliothèques complémentaires\n",
        "from datetime import datetime   # Manipulation de dates\n",
        "import warnings                 # Gestion des avertissements\n",
        "from scipy import stats        # Tests statistiques\n",
        "\n",
        "# Configuration de l'affichage des graphiques\n",
        "plt.style.use('seaborn-v0_8-whitegrid')  # Style professionnel pour les graphiques\n",
        "sns.set_palette(\"Set2\")                   # Palette de couleurs professionnelle\n",
        "\n",
        "# Configuration de la taille par défaut des figures\n",
        "plt.rcParams['figure.figsize'] = (14, 8)  # Largeur: 14 pouces, Hauteur: 8 pouces\n",
        "plt.rcParams['font.size'] = 11            # Taille de police par défaut\n",
        "plt.rcParams['axes.labelsize'] = 12       # Taille des labels d'axes\n",
        "plt.rcParams['axes.titlesize'] = 14       # Taille des titres\n",
        "plt.rcParams['xtick.labelsize'] = 10      # Taille des ticks X\n",
        "plt.rcParams['ytick.labelsize'] = 10      # Taille des ticks Y\n",
        "plt.rcParams['legend.fontsize'] = 10      # Taille de la légende\n",
        "\n",
        "# Suppression des avertissements non critiques pour une sortie propre\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration de l'affichage pandas\n",
        "pd.set_option('display.max_columns', None)          # Afficher toutes les colonnes\n",
        "pd.set_option('display.max_rows', 100)              # Afficher jusqu'à 100 lignes\n",
        "pd.set_option('display.precision', 2)               # 2 décimales pour les nombres\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)  # Format des floats\n",
        "pd.set_option('display.width', 120)                 # Largeur d'affichage\n",
        "\n",
        "# Affichage des informations de configuration\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION DE L'ENVIRONNEMENT D'ANALYSE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✓ Python: {sys.version.split()[0]}\")\n",
        "print(f\"✓ pandas: {pd.__version__}\")\n",
        "print(f\"✓ numpy: {np.__version__}\")\n",
        "print(f\"✓ matplotlib: {matplotlib.__version__}\") # Corrected to use matplotlib.__version__\n",
        "print(f\"✓ seaborn: {sns.__version__}\")\n",
        "print(f\"✓ Date d'analyse: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n",
        "print(\"✓ Environnement configuré avec succès\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JICMSarWw_dh",
        "outputId": "95959697-8c31-4874-fa5a-d4f0d389edb1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONFIGURATION DE L'ENVIRONNEMENT D'ANALYSE\n",
            "================================================================================\n",
            "✓ Python: 3.12.12\n",
            "✓ pandas: 2.2.2\n",
            "✓ numpy: 2.0.2\n",
            "✓ matplotlib: 3.10.0\n",
            "✓ seaborn: 0.13.2\n",
            "✓ Date d'analyse: 2025-11-13 10:32:51\n",
            "================================================================================\n",
            "✓ Environnement configuré avec succès\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CHARGEMENT ET EXPLORATION INITIALE DES DONNÉES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CHARGEMENT DES DONNÉES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Définition des noms des colonnes (A1 à A16)\n",
        "# Les noms sont anonymisés dans le dataset original\n",
        "column_names = [f'A{i}' for i in range(1, 17)]\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "# Note: Le fichier utilise '?' comme indicateur de valeur manquante\n",
        "try:\n",
        "    df = pd.read_csv('credit_approval.csv',\n",
        "                     names=column_names,        # Noms des colonnes\n",
        "                     na_values='?',             # Marqueur de valeurs manquantes\n",
        "                     skipinitialspace=True)     # Supprimer les espaces initiaux\n",
        "    print(f\"✓ Fichier chargé avec succès\")\n",
        "    print(f\"✓ Dimensions du dataset: {df.shape[0]} lignes × {df.shape[1]} colonnes\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠ Fichier non trouvé. Création d'un dataset synthétique pour démonstration...\")\n",
        "    # Création d'un dataset synthétique pour la démonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 690\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'A1': np.random.choice(['a', 'b'], n_samples),\n",
        "        'A2': np.random.uniform(15, 75, n_samples),\n",
        "        'A3': np.random.uniform(0, 20, n_samples),\n",
        "        'A4': np.random.choice(['u', 'y', 'l', 't'], n_samples),\n",
        "        'A5': np.random.choice(['g', 'p', 'gg'], n_samples),\n",
        "        'A6': np.random.choice(['c', 'w', 'q', 'k', 'i', 'aa', 'ff', 'x'], n_samples),\n",
        "        'A7': np.random.choice(['v', 'h', 'bb', 'z', 'ff'], n_samples),\n",
        "        'A8': np.random.uniform(0, 15, n_samples),\n",
        "        'A9': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A10': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A11': np.random.randint(0, 30, n_samples),\n",
        "        'A12': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A13': np.random.choice(['g', 'p', 's'], n_samples),\n",
        "        'A14': np.random.uniform(0, 1000, n_samples),\n",
        "        'A15': np.random.randint(0, 50000, n_samples),\n",
        "        'A16': np.random.choice(['+', '-'], n_samples, p=[0.55, 0.45])\n",
        "    })\n",
        "\n",
        "    # Introduction aléatoire de valeurs manquantes (5% du dataset)\n",
        "    for col in df.columns[:-1]:  # Sauf A16 (variable cible)\n",
        "        mask = np.random.random(n_samples) < 0.05\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "    print(f\"✓ Dataset synthétique créé: {df.shape[0]} lignes × {df.shape[1]} colonnes\")\n",
        "\n",
        "# Affichage des premières lignes\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"APERÇU DES PREMIÈRES LIGNES DU DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.head(10))\n",
        "\n",
        "# Affichage des dernières lignes pour vérifier la cohérence\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"APERÇU DES DERNIÈRES LIGNES DU DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.tail(5))\n",
        "\n",
        "# Informations générales sur le dataset\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"INFORMATIONS GÉNÉRALES SUR LE DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.info())\n",
        "\n",
        "# Types de données par colonne\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n✓ Exploration initiale terminée\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6UiF8XvyDCf",
        "outputId": "bf3f22f5-8a64-4e18-9dcf-fa536b18c14c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CHARGEMENT DES DONNÉES\n",
            "================================================================================\n",
            "⚠ Fichier non trouvé. Création d'un dataset synthétique pour démonstration...\n",
            "✓ Dataset synthétique créé: 690 lignes × 16 colonnes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "APERÇU DES PREMIÈRES LIGNES DU DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "  A1    A2    A3   A4   A5   A6  A7    A8 A9  A10   A11  A12 A13    A14      A15 A16\n",
            "0  a 51.95 13.93    t    p  NaN   v  6.63  f    t 15.00    t   g 520.23 28264.00   -\n",
            "1  b 53.11  6.80    u    p    c  bb 10.61  f    f 27.00    f   p  37.11      NaN   +\n",
            "2  a 17.72   NaN    t   gg    k   z  5.84  t    t  0.00    t   p  80.67 41473.00   -\n",
            "3  a 37.48  1.31    t    g    c   v  3.43  f    f  3.00    f   s 672.35 22375.00   -\n",
            "4  a 52.55  6.31    l   gg    i  bb  8.95  t    f  9.00    f   g 689.18 22611.00   +\n",
            "5  b   NaN 10.79    t    p    c   z 13.92  t    t 10.00    f   s 341.47 42026.00   +\n",
            "6  a 66.39 15.81    u    p   ff   h   NaN  f  NaN 16.00    f   p 215.61  7833.00   +\n",
            "7  a 54.52  6.38    u   gg    i  bb  5.13  f    t 24.00    f   p 813.71 47013.00   -\n",
            "8  a   NaN 12.52  NaN  NaN    i  ff  7.92  f    t 26.00  NaN   p 217.10 15754.00   +\n",
            "9  b 19.23   NaN    t    g    x   v  3.17  f    f 22.00    t   g 542.03 42048.00   -\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "APERÇU DES DERNIÈRES LIGNES DU DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "    A1    A2    A3 A4   A5  A6  A7    A8   A9  A10   A11 A12 A13    A14      A15 A16\n",
            "685  b 17.22  2.84  u  NaN  ff   v  2.15    t    t 25.00   t   g 189.56 19420.00   +\n",
            "686  b 50.78  2.43  t    g   w   z 10.72  NaN    t  4.00   f   s 892.24      NaN   +\n",
            "687  a 28.80  6.07  y   gg   k  ff  4.40    f  NaN 15.00   t   p 430.94 40366.00   -\n",
            "688  a 22.23  2.02  l    p   i  ff  7.88    f    f  0.00   t   s 198.00 37606.00   +\n",
            "689  b 19.62 13.84  t   gg   i  bb   NaN    t    f 27.00   f   p 438.02 38640.00   -\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "INFORMATIONS GÉNÉRALES SUR LE DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 690 entries, 0 to 689\n",
            "Data columns (total 16 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   A1      651 non-null    object \n",
            " 1   A2      652 non-null    float64\n",
            " 2   A3      651 non-null    float64\n",
            " 3   A4      653 non-null    object \n",
            " 4   A5      648 non-null    object \n",
            " 5   A6      658 non-null    object \n",
            " 6   A7      652 non-null    object \n",
            " 7   A8      664 non-null    float64\n",
            " 8   A9      653 non-null    object \n",
            " 9   A10     647 non-null    object \n",
            " 10  A11     668 non-null    float64\n",
            " 11  A12     651 non-null    object \n",
            " 12  A13     661 non-null    object \n",
            " 13  A14     652 non-null    float64\n",
            " 14  A15     657 non-null    float64\n",
            " 15  A16     690 non-null    object \n",
            "dtypes: float64(6), object(10)\n",
            "memory usage: 86.4+ KB\n",
            "None\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "A1      object\n",
            "A2     float64\n",
            "A3     float64\n",
            "A4      object\n",
            "A5      object\n",
            "A6      object\n",
            "A7      object\n",
            "A8     float64\n",
            "A9      object\n",
            "A10     object\n",
            "A11    float64\n",
            "A12     object\n",
            "A13     object\n",
            "A14    float64\n",
            "A15    float64\n",
            "A16     object\n",
            "dtype: object\n",
            "\n",
            "✓ Exploration initiale terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IDENTIFICATION ET CONVERSION DES TYPES DE DONNÉES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IDENTIFICATION DES TYPES DE VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Définition manuelle des types de variables basée sur la documentation\n",
        "variables_continues = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
        "variables_categorielles = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
        "variable_cible = 'A16'\n",
        "\n",
        "print(f\"\\n✓ Variables continues: {len(variables_continues)}\")\n",
        "print(f\"  {', '.join(variables_continues)}\")\n",
        "\n",
        "print(f\"\\n✓ Variables catégorielles: {len(variables_categorielles)}\")\n",
        "print(f\"  {', '.join(variables_categorielles)}\")\n",
        "\n",
        "print(f\"\\n✓ Variable cible: {variable_cible}\")\n",
        "\n",
        "# Conversion des types de données\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"CONVERSION DES TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Conversion des variables continues en float\n",
        "for col in variables_continues:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        print(f\"✓ {col}: converti en numérique (float)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ {col}: erreur de conversion - {str(e)}\")\n",
        "\n",
        "# Conversion des variables catégorielles en type 'category'\n",
        "for col in variables_categorielles + [variable_cible]:\n",
        "    try:\n",
        "        df[col] = df[col].astype('category')\n",
        "        print(f\"✓ {col}: converti en catégoriel ({df[col].nunique()} catégories)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ {col}: erreur de conversion - {str(e)}\")\n",
        "\n",
        "# Vérification des conversions\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TYPES DE DONNÉES APRÈS CONVERSION\")\n",
        "print(\"-\"*80)\n",
        "print(df.dtypes)\n",
        "\n",
        "# Résumé des types\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"RÉSUMÉ DES TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Variables numériques (float64): {(df.dtypes == 'float64').sum()}\")\n",
        "print(f\"Variables catégorielles (category): {(df.dtypes == 'category').sum()}\")\n",
        "print(f\"Autres types: {((df.dtypes != 'float64') & (df.dtypes != 'category')).sum()}\")\n",
        "\n",
        "print(\"\\n✓ Conversion des types terminée\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id5rinnmyLh_",
        "outputId": "a674cbe4-79e9-410e-981d-03407f261e19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "IDENTIFICATION DES TYPES DE VARIABLES\n",
            "================================================================================\n",
            "\n",
            "✓ Variables continues: 6\n",
            "  A2, A3, A8, A11, A14, A15\n",
            "\n",
            "✓ Variables catégorielles: 9\n",
            "  A1, A4, A5, A6, A7, A9, A10, A12, A13\n",
            "\n",
            "✓ Variable cible: A16\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CONVERSION DES TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "✓ A2: converti en numérique (float)\n",
            "✓ A3: converti en numérique (float)\n",
            "✓ A8: converti en numérique (float)\n",
            "✓ A11: converti en numérique (float)\n",
            "✓ A14: converti en numérique (float)\n",
            "✓ A15: converti en numérique (float)\n",
            "✓ A1: converti en catégoriel (2 catégories)\n",
            "✓ A4: converti en catégoriel (4 catégories)\n",
            "✓ A5: converti en catégoriel (3 catégories)\n",
            "✓ A6: converti en catégoriel (8 catégories)\n",
            "✓ A7: converti en catégoriel (5 catégories)\n",
            "✓ A9: converti en catégoriel (2 catégories)\n",
            "✓ A10: converti en catégoriel (2 catégories)\n",
            "✓ A12: converti en catégoriel (2 catégories)\n",
            "✓ A13: converti en catégoriel (3 catégories)\n",
            "✓ A16: converti en catégoriel (2 catégories)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TYPES DE DONNÉES APRÈS CONVERSION\n",
            "--------------------------------------------------------------------------------\n",
            "A1     category\n",
            "A2      float64\n",
            "A3      float64\n",
            "A4     category\n",
            "A5     category\n",
            "A6     category\n",
            "A7     category\n",
            "A8      float64\n",
            "A9     category\n",
            "A10    category\n",
            "A11     float64\n",
            "A12    category\n",
            "A13    category\n",
            "A14     float64\n",
            "A15     float64\n",
            "A16    category\n",
            "dtype: object\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "RÉSUMÉ DES TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "Variables numériques (float64): 6\n",
            "Variables catégorielles (category): 10\n",
            "Autres types: 0\n",
            "\n",
            "✓ Conversion des types terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ANALYSE DÉTAILLÉE DES VALEURS MANQUANTES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSE DES VALEURS MANQUANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcul des statistiques de valeurs manquantes\n",
        "valeurs_manquantes = df.isnull().sum()\n",
        "pourcentage_manquant = (valeurs_manquantes / len(df)) * 100\n",
        "\n",
        "# Création d'un DataFrame récapitulatif\n",
        "missing_df = pd.DataFrame({\n",
        "    'Variable': df.columns,\n",
        "    'Valeurs_manquantes': valeurs_manquantes.values,\n",
        "    'Pourcentage': pourcentage_manquant.values,\n",
        "    'Type': df.dtypes.values\n",
        "}).sort_values('Pourcentage', ascending=False)\n",
        "\n",
        "# Affichage du tableau\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TABLEAU RÉCAPITULATIF DES VALEURS MANQUANTES\")\n",
        "print(\"-\"*80)\n",
        "print(missing_df.to_string(index=False))\n",
        "\n",
        "# Statistiques globales\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STATISTIQUES GLOBALES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Nombre total de cellules: {df.size:,}\")\n",
        "print(f\"Nombre de cellules avec valeurs manquantes: {df.isnull().sum().sum():,}\")\n",
        "print(f\"Pourcentage global de valeurs manquantes: {(df.isnull().sum().sum() / df.size) * 100:.2f}%\")\n",
        "\n",
        "# Identification des variables problématiques (>10% de données manquantes)\n",
        "variables_problematiques = missing_df[missing_df['Pourcentage'] > 10]\n",
        "if len(variables_problematiques) > 0:\n",
        "    print(f\"\\n⚠ {len(variables_problematiques)} variable(s) avec >10% de données manquantes:\")\n",
        "    for idx, row in variables_problematiques.iterrows():\n",
        "        print(f\"  - {row['Variable']}: {row['Pourcentage']:.2f}%\")\n",
        "else:\n",
        "    print(\"\\n✓ Aucune variable avec plus de 10% de données manquantes\")\n",
        "\n",
        "# Analyse des lignes avec valeurs manquantes\n",
        "lignes_completes = df.dropna().shape[0]\n",
        "lignes_avec_manquantes = df.shape[0] - lignes_completes\n",
        "\n",
        "print(f\"\\nLignes complètes (sans valeurs manquantes): {lignes_completes} ({(lignes_completes/len(df))*100:.2f}%)\")\n",
        "print(f\"Lignes avec au moins une valeur manquante: {lignes_avec_manquantes} ({(lignes_avec_manquantes/len(df))*100:.2f}%)\")\n",
        "\n",
        "# Distribution du nombre de valeurs manquantes par ligne\n",
        "missing_per_row = df.isnull().sum(axis=1)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DISTRIBUTION DU NOMBRE DE VALEURS MANQUANTES PAR LIGNE\")\n",
        "print(\"-\"*80)\n",
        "print(missing_per_row.value_counts().sort_index())\n",
        "\n",
        "# Test de pattern de données manquantes (MCAR, MAR, MNAR)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ANALYSE DES PATTERNS DE DONNÉES MANQUANTES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Corrélation entre les données manquantes\n",
        "missing_matrix = df.isnull().astype(int)\n",
        "missing_corr = missing_matrix.corr()\n",
        "\n",
        "# Identifier les paires de variables avec forte corrélation de manquance\n",
        "high_corr_missing = []\n",
        "for i in range(len(missing_corr.columns)):\n",
        "    for j in range(i+1, len(missing_corr.columns)):\n",
        "        if abs(missing_corr.iloc[i, j]) > 0.3 and missing_corr.iloc[i, j] != 1:\n",
        "            high_corr_missing.append({\n",
        "                'Var1': missing_corr.columns[i],\n",
        "                'Var2': missing_corr.columns[j],\n",
        "                'Corrélation': missing_corr.iloc[i, j]\n",
        "            })\n",
        "\n",
        "if high_corr_missing:\n",
        "    print(\"\\n⚠ Corrélations significatives détectées entre les patterns de données manquantes:\")\n",
        "    print(\"   (Suggère que les données ne sont peut-être pas MCAR)\")\n",
        "    for item in high_corr_missing:\n",
        "        print(f\"  - {item['Var1']} ↔ {item['Var2']}: r = {item['Corrélation']:.3f}\")\n",
        "else:\n",
        "    print(\"\\n✓ Pas de corrélation forte entre les patterns de données manquantes\")\n",
        "    print(\"   (Compatible avec l'hypothèse MCAR - Missing Completely At Random)\")\n",
        "\n",
        "print(\"\\n✓ Analyse des valeurs manquantes terminée\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJIQxsKhykTP",
        "outputId": "0bad2f59-4da0-481b-b388-5c696bc27cec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSE DES VALEURS MANQUANTES\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TABLEAU RÉCAPITULATIF DES VALEURS MANQUANTES\n",
            "--------------------------------------------------------------------------------\n",
            "Variable  Valeurs_manquantes  Pourcentage     Type\n",
            "     A10                  43         6.23 category\n",
            "      A5                  42         6.09 category\n",
            "      A3                  39         5.65  float64\n",
            "      A1                  39         5.65 category\n",
            "     A12                  39         5.65 category\n",
            "     A14                  38         5.51  float64\n",
            "      A7                  38         5.51 category\n",
            "      A2                  38         5.51  float64\n",
            "      A4                  37         5.36 category\n",
            "      A9                  37         5.36 category\n",
            "     A15                  33         4.78  float64\n",
            "      A6                  32         4.64 category\n",
            "     A13                  29         4.20 category\n",
            "      A8                  26         3.77  float64\n",
            "     A11                  22         3.19  float64\n",
            "     A16                   0         0.00 category\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STATISTIQUES GLOBALES\n",
            "--------------------------------------------------------------------------------\n",
            "Nombre total de cellules: 11,040\n",
            "Nombre de cellules avec valeurs manquantes: 532\n",
            "Pourcentage global de valeurs manquantes: 4.82%\n",
            "\n",
            "✓ Aucune variable avec plus de 10% de données manquantes\n",
            "\n",
            "Lignes complètes (sans valeurs manquantes): 315 (45.65%)\n",
            "Lignes avec au moins une valeur manquante: 375 (54.35%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISTRIBUTION DU NOMBRE DE VALEURS MANQUANTES PAR LIGNE\n",
            "--------------------------------------------------------------------------------\n",
            "0    315\n",
            "1    262\n",
            "2     78\n",
            "3     26\n",
            "4      9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ANALYSE DES PATTERNS DE DONNÉES MANQUANTES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "✓ Pas de corrélation forte entre les patterns de données manquantes\n",
            "   (Compatible avec l'hypothèse MCAR - Missing Completely At Random)\n",
            "\n",
            "✓ Analyse des valeurs manquantes terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAITEMENT DES VALEURS MANQUANTES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAITEMENT DES VALEURS MANQUANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Création d'une copie du DataFrame pour le traitement\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STRATÉGIES DE TRAITEMENT PAR TYPE DE VARIABLE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# 1. TRAITEMENT DES VARIABLES CONTINUES\n",
        "print(\"\\n1. Variables continues - Imputation par la médiane\")\n",
        "print(\"   (Plus robuste aux outliers que la moyenne)\")\n",
        "\n",
        "for col in variables_continues:\n",
        "    missing_count = df_cleaned[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        # Calcul de la médiane avant imputation\n",
        "        mediane = df_cleaned[col].median()\n",
        "        # Imputation\n",
        "        df_cleaned[col].fillna(mediane, inplace=True)\n",
        "        print(f\"   ✓ {col}: {missing_count} valeurs imputées avec la médiane ({mediane:.2f})\")\n",
        "    else:\n",
        "        print(f\"   ✓ {col}: aucune valeur manquante\")\n",
        "\n",
        "# 2. TRAITEMENT DES VARIABLES CATÉGORIELLES\n",
        "print(\"\\n2. Variables catégorielles - Imputation par le mode\")\n",
        "print(\"   (Valeur la plus fréquente)\")\n",
        "\n",
        "for col in variables_categorielles:\n",
        "    missing_count = df_cleaned[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        # Calcul du mode avant imputation\n",
        "        mode_value = df_cleaned[col].mode()[0]\n",
        "        # Imputation\n",
        "        df_cleaned[col].fillna(mode_value, inplace=True)\n",
        "        print(f\"   ✓ {col}: {missing_count} valeurs imputées avec le mode ('{mode_value}')\")\n",
        "    else:\n",
        "        print(f\"   ✓ {col}: aucune valeur manquante\")\n",
        "\n",
        "# 3. VÉRIFICATION FINALE DE L'ABSENCE DE VALEURS MANQUANTES\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"VÉRIFICATION FINALE DES VALEURS MANQUANTES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "if df_cleaned.isnull().sum().sum() == 0:\n",
        "    print(\"✓ Toutes les valeurs manquantes ont été traitées avec succès.\")\n",
        "else:\n",
        "    print(f\"⚠ Il reste {df_cleaned.isnull().sum().sum()} valeurs manquantes après traitement.\")\n",
        "\n",
        "print(\"\\n✓ Traitement des valeurs manquantes terminé.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhrrNk2Wyqf9",
        "outputId": "0fe95c72-3306-49c2-ed89-794701778cfa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAITEMENT DES VALEURS MANQUANTES\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STRATÉGIES DE TRAITEMENT PAR TYPE DE VARIABLE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Variables continues - Imputation par la médiane\n",
            "   (Plus robuste aux outliers que la moyenne)\n",
            "   ✓ A2: 38 valeurs imputées avec la médiane (44.84)\n",
            "   ✓ A3: 39 valeurs imputées avec la médiane (10.38)\n",
            "   ✓ A8: 26 valeurs imputées avec la médiane (7.04)\n",
            "   ✓ A11: 22 valeurs imputées avec la médiane (14.50)\n",
            "   ✓ A14: 38 valeurs imputées avec la médiane (520.16)\n",
            "   ✓ A15: 33 valeurs imputées avec la médiane (24722.00)\n",
            "\n",
            "2. Variables catégorielles - Imputation par le mode\n",
            "   (Valeur la plus fréquente)\n",
            "   ✓ A1: 39 valeurs imputées avec le mode ('b')\n",
            "   ✓ A4: 37 valeurs imputées avec le mode ('t')\n",
            "   ✓ A5: 42 valeurs imputées avec le mode ('g')\n",
            "   ✓ A6: 32 valeurs imputées avec le mode ('k')\n",
            "   ✓ A7: 38 valeurs imputées avec le mode ('bb')\n",
            "   ✓ A9: 37 valeurs imputées avec le mode ('t')\n",
            "   ✓ A10: 43 valeurs imputées avec le mode ('t')\n",
            "   ✓ A12: 39 valeurs imputées avec le mode ('t')\n",
            "   ✓ A13: 29 valeurs imputées avec le mode ('p')\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "VÉRIFICATION FINALE DES VALEURS MANQUANTES\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Toutes les valeurs manquantes ont été traitées avec succès.\n",
            "\n",
            "✓ Traitement des valeurs manquantes terminé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ff6370"
      },
      "source": [
        "### Installation des bibliothèques nécessaires\n",
        "\n",
        "Ce bloc de code installe la bibliothèque `ucimlrepo` qui permet d'accéder facilement à des jeux de données hébergés sur le référentiel UCI Machine Learning, y compris le jeu de données 'Credit Approval'. Il assure que toutes les dépendances sont satisfaites avant de procéder à l'analyse des données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea80040",
        "outputId": "e16b0019-c8d3-4abc-8621-7fbff55c9ac5"
      },
      "source": [
        "!pip install ucimlrepo"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de02480c"
      },
      "source": [
        "### Chargement du jeu de données \"Credit Approval\"\n",
        "\n",
        "Ce bloc de code utilise la bibliothèque `ucimlrepo` pour charger le jeu de données 'Credit Approval' (ID 27). Il extrait les caractéristiques (features) dans `X` et la variable cible (targets) dans `y`. Ensuite, il affiche les métadonnées et les informations sur les variables du jeu de données, ce qui est crucial pour comprendre la nature et la structure des données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b5f993",
        "outputId": "fde43a4c-543c-4d12-d393-ae6c90d66070"
      },
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "credit_approval = fetch_ucirepo(id=27)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = credit_approval.data.features\n",
        "y = credit_approval.data.targets\n",
        "\n",
        "# metadata\n",
        "print(credit_approval.metadata)\n",
        "\n",
        "# variable information\n",
        "print(credit_approval.variables)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 27, 'name': 'Credit Approval', 'repository_url': 'https://archive.ics.uci.edu/dataset/27/credit+approval', 'data_url': 'https://archive.ics.uci.edu/static/public/27/data.csv', 'abstract': 'This data concerns credit card applications; good mix of attributes', 'area': 'Business', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 690, 'num_features': 15, 'feature_types': ['Categorical', 'Integer', 'Real'], 'demographics': [], 'target_col': ['A16'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1987, 'last_updated': 'Wed Aug 23 2023', 'dataset_doi': '10.24432/C5FS30', 'creators': ['J. R. Quinlan'], 'intro_paper': None, 'additional_info': {'summary': 'This file concerns credit card applications.  All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.\\r\\n  \\r\\nThis dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values.  There are also a few missing values.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'A1:\\tb, a.\\r\\nA2:\\tcontinuous.\\r\\nA3:\\tcontinuous.\\r\\nA4:\\tu, y, l, t.\\r\\nA5:\\tg, p, gg.\\r\\nA6:\\tc, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\\r\\nA7:\\tv, h, bb, j, n, z, dd, ff, o.\\r\\nA8:\\tcontinuous.\\r\\nA9:\\tt, f.\\r\\nA10:\\tt, f.\\r\\nA11:\\tcontinuous.\\r\\nA12:\\tt, f.\\r\\nA13:\\tg, p, s.\\r\\nA14:\\tcontinuous.\\r\\nA15:\\tcontinuous.\\r\\nA16: +,-         (class attribute)', 'citation': None}}\n",
            "   name     role         type demographic description units missing_values\n",
            "0   A16   Target  Categorical        None        None  None             no\n",
            "1   A15  Feature   Continuous        None        None  None             no\n",
            "2   A14  Feature   Continuous        None        None  None            yes\n",
            "3   A13  Feature  Categorical        None        None  None             no\n",
            "4   A12  Feature  Categorical        None        None  None             no\n",
            "5   A11  Feature   Continuous        None        None  None             no\n",
            "6   A10  Feature  Categorical        None        None  None             no\n",
            "7    A9  Feature  Categorical        None        None  None             no\n",
            "8    A8  Feature   Continuous        None        None  None             no\n",
            "9    A7  Feature  Categorical        None        None  None            yes\n",
            "10   A6  Feature  Categorical        None        None  None            yes\n",
            "11   A5  Feature  Categorical        None        None  None            yes\n",
            "12   A4  Feature  Categorical        None        None  None            yes\n",
            "13   A3  Feature   Continuous        None        None  None             no\n",
            "14   A2  Feature   Continuous        None        None  None            yes\n",
            "15   A1  Feature  Categorical        None        None  None            yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2abcd65f"
      },
      "source": [
        "### Configuration de l'environnement d'analyse\n",
        "\n",
        "Ce bloc de code importe les bibliothèques Python couramment utilisées pour l'analyse de données (pandas, numpy, matplotlib, seaborn, etc.) et configure divers paramètres pour améliorer la lisibilité et l'esthétique des visualisations et des affichages de DataFrames. Cela inclut la définition d'un style de graphique, d'une palette de couleurs, des tailles de police par défaut et des options d'affichage pour pandas, garantissant un environnement de travail cohérent et agréable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b858ff8",
        "outputId": "29624344-bd5c-43e2-aa2b-9d0ffb3a7b7d"
      },
      "source": [
        "import pandas as pd              # Manipulation et analyse de données tabulaires\n",
        "import numpy as np               # Calculs numériques et opérations sur des arrays\n",
        "import sys                       # Importation du module sys pour les informations système\n",
        "import matplotlib                # Importation du module matplotlib entier\n",
        "\n",
        "# Importation des bibliothèques pour la visualisation\n",
        "import matplotlib.pyplot as plt  # Création de graphiques de base\n",
        "import seaborn as sns           # Visualisations statistiques avancées\n",
        "\n",
        "# Importation de bibliothèques complémentaires\n",
        "from datetime import datetime   # Manipulation de dates\n",
        "import warnings                 # Gestion des avertissements\n",
        "from scipy import stats        # Tests statistiques\n",
        "\n",
        "# Configuration de l'affichage des graphiques\n",
        "plt.style.use('seaborn-v0_8-whitegrid')  # Style professionnel pour les graphiques\n",
        "sns.set_palette(\"Set2\")                   # Palette de couleurs professionnelle\n",
        "\n",
        "# Configuration de la taille par défaut des figures\n",
        "plt.rcParams['figure.figsize'] = (14, 8)  # Largeur: 14 pouces, Hauteur: 8 pouces\n",
        "plt.rcParams['font.size'] = 11            # Taille de police par défaut\n",
        "plt.rcParams['axes.labelsize'] = 12       # Taille des labels d'axes\n",
        "plt.rcParams['axes.titlesize'] = 14       # Taille des titres\n",
        "plt.rcParams['xtick.labelsize'] = 10      # Taille des ticks X\n",
        "plt.rcParams['ytick.labelsize'] = 10      # Taille des ticks Y\n",
        "plt.rcParams['legend.fontsize'] = 10      # Taille de la légende\n",
        "\n",
        "# Suppression des avertissements non critiques pour une sortie propre\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration de l'affichage pandas\n",
        "pd.set_option('display.max_columns', None)          # Afficher toutes les colonnes\n",
        "pd.set_option('display.max_rows', 100)              # Afficher jusqu'à 100 lignes\n",
        "pd.set_option('display.precision', 2)               # 2 décimales pour les nombres\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)  # Format des floats\n",
        "pd.set_option('display.width', 120)                 # Largeur d'affichage\n",
        "\n",
        "# Affichage des informations de configuration\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION DE L'ENVIRONNEMENT D'ANALYSE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"✓ Python: {sys.version.split()[0]}\")\n",
        "print(f\"✓ pandas: {pd.__version__}\")\n",
        "print(f\"✓ numpy: {np.__version__}\")\n",
        "print(f\"✓ matplotlib: {matplotlib.__version__}\")\n",
        "print(f\"✓ seaborn: {sns.__version__}\")\n",
        "print(f\"✓ Date d'analyse: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)\n",
        "print(\"✓ Environnement configuré avec succès\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONFIGURATION DE L'ENVIRONNEMENT D'ANALYSE\n",
            "================================================================================\n",
            "✓ Python: 3.12.12\n",
            "✓ pandas: 2.2.2\n",
            "✓ numpy: 2.0.2\n",
            "✓ matplotlib: 3.10.0\n",
            "✓ seaborn: 0.13.2\n",
            "✓ Date d'analyse: 2025-11-13 10:42:34\n",
            "================================================================================\n",
            "✓ Environnement configuré avec succès\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ec85c2b"
      },
      "source": [
        "### Chargement et exploration initiale des données\n",
        "\n",
        "Ce bloc de code gère le chargement du jeu de données. Si le fichier `credit_approval.csv` n'est pas trouvé (ce qui est souvent le cas dans un environnement Colab sans upload manuel), il génère un jeu de données synthétique avec une structure similaire pour permettre la poursuite de l'analyse. Il affiche ensuite les premières et dernières lignes du DataFrame, ainsi que des informations générales (`df.info()`) et les types de données (`df.dtypes`) pour une première compréhension de la structure et de la qualité des données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decd6512",
        "outputId": "7d648872-ce27-4d59-bdf1-bd85c8b10842"
      },
      "source": [
        "# ============================================================================\n",
        "# CHARGEMENT ET EXPLORATION INITIALE DES DONNÉES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CHARGEMENT DES DONNÉES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Définition des noms des colonnes (A1 à A16)\n",
        "# Les noms sont anonymisés dans le dataset original\n",
        "column_names = [f'A{i}' for i in range(1, 17)]\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "# Note: Le fichier utilise '?' comme indicateur de valeur manquante\n",
        "try:\n",
        "    df = pd.read_csv('credit_approval.csv',\n",
        "                     names=column_names,        # Noms des colonnes\n",
        "                     na_values='?',             # Marqueur de valeurs manquantes\n",
        "                     skipinitialspace=True)     # Supprimer les espaces initiaux\n",
        "    print(f\"✓ Fichier chargé avec succès\")\n",
        "    print(f\"✓ Dimensions du dataset: {df.shape[0]} lignes × {df.shape[1]} colonnes\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠ Fichier non trouvé. Création d'un dataset synthétique pour démonstration...\")\n",
        "    # Création d'un dataset synthétique pour la démonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 690\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'A1': np.random.choice(['a', 'b'], n_samples),\n",
        "        'A2': np.random.uniform(15, 75, n_samples),\n",
        "        'A3': np.random.uniform(0, 20, n_samples),\n",
        "        'A4': np.random.choice(['u', 'y', 'l', 't'], n_samples),\n",
        "        'A5': np.random.choice(['g', 'p', 'gg'], n_samples),\n",
        "        'A6': np.random.choice(['c', 'w', 'q', 'k', 'i', 'aa', 'ff', 'x'], n_samples),\n",
        "        'A7': np.random.choice(['v', 'h', 'bb', 'z', 'ff'], n_samples),\n",
        "        'A8': np.random.uniform(0, 15, n_samples),\n",
        "        'A9': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A10': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A11': np.random.randint(0, 30, n_samples),\n",
        "        'A12': np.random.choice(['t', 'f'], n_samples),\n",
        "        'A13': np.random.choice(['g', 'p', 's'], n_samples),\n",
        "        'A14': np.random.uniform(0, 1000, n_samples),\n",
        "        'A15': np.random.randint(0, 50000, n_samples),\n",
        "        'A16': np.random.choice(['+', '-'], n_samples, p=[0.55, 0.45])\n",
        "    })\n",
        "\n",
        "    # Introduction aléatoire de valeurs manquantes (5% du dataset)\n",
        "    for col in df.columns[:-1]:  # Sauf A16 (variable cible)\n",
        "        mask = np.random.random(n_samples) < 0.05\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "    print(f\"✓ Dataset synthétique créé: {df.shape[0]} lignes × {df.shape[1]} colonnes\")\n",
        "\n",
        "# Affichage des premières lignes\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"APERÇU DES PREMIÈRES LIGNES DU DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.head(10))\n",
        "\n",
        "# Affichage des dernières lignes pour vérifier la cohérence\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"APERÇU DES DERNIÈRES LIGNES DU DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.tail(5))\n",
        "\n",
        "# Informations générales sur le dataset\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"INFORMATIONS GÉNÉRALES SUR LE DATASET\")\n",
        "print(\"-\"*80)\n",
        "print(df.info())\n",
        "\n",
        "# Types de données par colonne\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n✓ Exploration initiale terminée\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CHARGEMENT DES DONNÉES\n",
            "================================================================================\n",
            "⚠ Fichier non trouvé. Création d'un dataset synthétique pour démonstration...\n",
            "✓ Dataset synthétique créé: 690 lignes × 16 colonnes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "APERÇU DES PREMIÈRES LIGNES DU DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "  A1    A2    A3   A4   A5   A6  A7    A8 A9  A10   A11  A12 A13    A14      A15 A16\n",
            "0  a 51.95 13.93    t    p  NaN   v  6.63  f    t 15.00    t   g 520.23 28264.00   -\n",
            "1  b 53.11  6.80    u    p    c  bb 10.61  f    f 27.00    f   p  37.11      NaN   +\n",
            "2  a 17.72   NaN    t   gg    k   z  5.84  t    t  0.00    t   p  80.67 41473.00   -\n",
            "3  a 37.48  1.31    t    g    c   v  3.43  f    f  3.00    f   s 672.35 22375.00   -\n",
            "4  a 52.55  6.31    l   gg    i  bb  8.95  t    f  9.00    f   g 689.18 22611.00   +\n",
            "5  b   NaN 10.79    t    p    c   z 13.92  t    t 10.00    f   s 341.47 42026.00   +\n",
            "6  a 66.39 15.81    u    p   ff   h   NaN  f  NaN 16.00    f   p 215.61  7833.00   +\n",
            "7  a 54.52  6.38    u   gg    i  bb  5.13  f    t 24.00    f   p 813.71 47013.00   -\n",
            "8  a   NaN 12.52  NaN  NaN    i  ff  7.92  f    t 26.00  NaN   p 217.10 15754.00   +\n",
            "9  b 19.23   NaN    t    g    x   v  3.17  f    f 22.00    t   g 542.03 42048.00   -\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "APERÇU DES DERNIÈRES LIGNES DU DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "    A1    A2    A3 A4   A5  A6  A7    A8   A9  A10   A11 A12 A13    A14      A15 A16\n",
            "685  b 17.22  2.84  u  NaN  ff   v  2.15    t    t 25.00   t   g 189.56 19420.00   +\n",
            "686  b 50.78  2.43  t    g   w   z 10.72  NaN    t  4.00   f   s 892.24      NaN   +\n",
            "687  a 28.80  6.07  y   gg   k  ff  4.40    f  NaN 15.00   t   p 430.94 40366.00   -\n",
            "688  a 22.23  2.02  l    p   i  ff  7.88    f    f  0.00   t   s 198.00 37606.00   +\n",
            "689  b 19.62 13.84  t   gg   i  bb   NaN    t    f 27.00   f   p 438.02 38640.00   -\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "INFORMATIONS GÉNÉRALES SUR LE DATASET\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 690 entries, 0 to 689\n",
            "Data columns (total 16 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   A1      651 non-null    object \n",
            " 1   A2      652 non-null    float64\n",
            " 2   A3      651 non-null    float64\n",
            " 3   A4      653 non-null    object \n",
            " 4   A5      648 non-null    object \n",
            " 5   A6      658 non-null    object \n",
            " 6   A7      652 non-null    object \n",
            " 7   A8      664 non-null    float64\n",
            " 8   A9      653 non-null    object \n",
            " 9   A10     647 non-null    object \n",
            " 10  A11     668 non-null    float64\n",
            " 11  A12     651 non-null    object \n",
            " 12  A13     661 non-null    object \n",
            " 13  A14     652 non-null    float64\n",
            " 14  A15     657 non-null    float64\n",
            " 15  A16     690 non-null    object \n",
            "dtypes: float64(6), object(10)\n",
            "memory usage: 86.4+ KB\n",
            "None\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "A1      object\n",
            "A2     float64\n",
            "A3     float64\n",
            "A4      object\n",
            "A5      object\n",
            "A6      object\n",
            "A7      object\n",
            "A8     float64\n",
            "A9      object\n",
            "A10     object\n",
            "A11    float64\n",
            "A12     object\n",
            "A13     object\n",
            "A14    float64\n",
            "A15    float64\n",
            "A16     object\n",
            "dtype: object\n",
            "\n",
            "✓ Exploration initiale terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "767bc929"
      },
      "source": [
        "### Identification et conversion des types de données\n",
        "\n",
        "Ce bloc de code est crucial pour la préparation des données. Il identifie manuellement les variables continues et catégorielles, ainsi que la variable cible (`A16`), en se basant sur la documentation ou une compréhension préalable du jeu de données. Ensuite, il convertit les types de données de ces colonnes : les variables continues sont converties en `float` (numérique) et les variables catégorielles (y compris la variable cible) en type `category` de pandas. Cette conversion est essentielle pour optimiser la mémoire, améliorer les performances de certaines opérations et garantir que les algorithmes de machine learning interprètent correctement les données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb98df5a",
        "outputId": "11acc5f7-94ca-4863-8123-b2ebfa1de17e"
      },
      "source": [
        "# ============================================================================\n",
        "# IDENTIFICATION ET CONVERSION DES TYPES DE DONNÉES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IDENTIFICATION DES TYPES DE VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Définition manuelle des types de variables basée sur la documentation\n",
        "variables_continues = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
        "variables_categorielles = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
        "variable_cible = 'A16'\n",
        "\n",
        "print(f\"\\n✓ Variables continues: {len(variables_continues)}\")\n",
        "print(f\"  {', '.join(variables_continues)}\")\n",
        "\n",
        "print(f\"\\n✓ Variables catégorielles: {len(variables_categorielles)}\")\n",
        "print(f\"  {', '.join(variables_categorielles)}\")\n",
        "\n",
        "print(f\"\\n✓ Variable cible: {variable_cible}\")\n",
        "\n",
        "# Conversion des types de données\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"CONVERSION DES TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Conversion des variables continues en float\n",
        "for col in variables_continues:\n",
        "    try:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        print(f\"✓ {col}: converti en numérique (float)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ {col}: erreur de conversion - {str(e)}\")\n",
        "\n",
        "# Conversion des variables catégorielles en type 'category'\n",
        "for col in variables_categorielles + [variable_cible]:\n",
        "    try:\n",
        "        df[col] = df[col].astype('category')\n",
        "        print(f\"✓ {col}: converti en catégoriel ({df[col].nunique()} catégories)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ {col}: erreur de conversion - {str(e)}\")\n",
        "\n",
        "# Vérification des conversions\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TYPES DE DONNÉES APRÈS CONVERSION\")\n",
        "print(\"-\"*80)\n",
        "print(df.dtypes)\n",
        "\n",
        "# Résumé des types\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"RÉSUMÉ DES TYPES DE DONNÉES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Variables numériques (float64): {(df.dtypes == 'float64').sum()}\")\n",
        "print(f\"Variables catégorielles (category): {(df.dtypes == 'category').sum()}\")\n",
        "print(f\"Autres types: {((df.dtypes != 'float64') & (df.dtypes != 'category')).sum()}\")\n",
        "\n",
        "print(\"\\n✓ Conversion des types terminée\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "IDENTIFICATION DES TYPES DE VARIABLES\n",
            "================================================================================\n",
            "\n",
            "✓ Variables continues: 6\n",
            "  A2, A3, A8, A11, A14, A15\n",
            "\n",
            "✓ Variables catégorielles: 9\n",
            "  A1, A4, A5, A6, A7, A9, A10, A12, A13\n",
            "\n",
            "✓ Variable cible: A16\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CONVERSION DES TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "✓ A2: converti en numérique (float)\n",
            "✓ A3: converti en numérique (float)\n",
            "✓ A8: converti en numérique (float)\n",
            "✓ A11: converti en numérique (float)\n",
            "✓ A14: converti en numérique (float)\n",
            "✓ A15: converti en numérique (float)\n",
            "✓ A1: converti en catégoriel (2 catégories)\n",
            "✓ A4: converti en catégoriel (4 catégories)\n",
            "✓ A5: converti en catégoriel (3 catégories)\n",
            "✓ A6: converti en catégoriel (8 catégories)\n",
            "✓ A7: converti en catégoriel (5 catégories)\n",
            "✓ A9: converti en catégoriel (2 catégories)\n",
            "✓ A10: converti en catégoriel (2 catégories)\n",
            "✓ A12: converti en catégoriel (2 catégories)\n",
            "✓ A13: converti en catégoriel (3 catégories)\n",
            "✓ A16: converti en catégoriel (2 catégories)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TYPES DE DONNÉES APRÈS CONVERSION\n",
            "--------------------------------------------------------------------------------\n",
            "A1     category\n",
            "A2      float64\n",
            "A3      float64\n",
            "A4     category\n",
            "A5     category\n",
            "A6     category\n",
            "A7     category\n",
            "A8      float64\n",
            "A9     category\n",
            "A10    category\n",
            "A11     float64\n",
            "A12    category\n",
            "A13    category\n",
            "A14     float64\n",
            "A15     float64\n",
            "A16    category\n",
            "dtype: object\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "RÉSUMÉ DES TYPES DE DONNÉES\n",
            "--------------------------------------------------------------------------------\n",
            "Variables numériques (float64): 6\n",
            "Variables catégorielles (category): 10\n",
            "Autres types: 0\n",
            "\n",
            "✓ Conversion des types terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c00de6"
      },
      "source": [
        "### Analyse détaillée des valeurs manquantes\n",
        "\n",
        "Ce bloc de code effectue une analyse approfondie des valeurs manquantes dans le jeu de données. Il calcule le nombre et le pourcentage de valeurs manquantes par colonne, crée un tableau récapitulatif, et fournit des statistiques globales sur la complétude du dataset. Il identifie également les variables ayant un pourcentage élevé de données manquantes (plus de 10%). Enfin, il explore la distribution des valeurs manquantes par ligne et tente d'analyser les patterns de manquance en calculant la corrélation entre les indicateurs de manquance des différentes variables. Cela permet de mieux comprendre la nature des valeurs manquantes et d'orienter les stratégies d'imputation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb0911e4",
        "outputId": "6b99604c-3272-4e97-dade-7a0035d4f1ed"
      },
      "source": [
        "# ============================================================================\n",
        "# ANALYSE DÉTAILLÉE DES VALEURS MANQUANTES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSE DES VALEURS MANQUANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcul des statistiques de valeurs manquantes\n",
        "valeurs_manquantes = df.isnull().sum()\n",
        "pourcentage_manquant = (valeurs_manquantes / len(df)) * 100\n",
        "\n",
        "# Création d'un DataFrame récapitulatif\n",
        "missing_df = pd.DataFrame({\n",
        "    'Variable': df.columns,\n",
        "    'Valeurs_manquantes': valeurs_manquantes.values,\n",
        "    'Pourcentage': pourcentage_manquant.values,\n",
        "    'Type': df.dtypes.values\n",
        "}).sort_values('Pourcentage', ascending=False)\n",
        "\n",
        "# Affichage du tableau\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TABLEAU RÉCAPITULATIF DES VALEURS MANQUANTES\")\n",
        "print(\"-\"*80)\n",
        "print(missing_df.to_string(index=False))\n",
        "\n",
        "# Statistiques globales\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STATISTIQUES GLOBALES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Nombre total de cellules: {df.size:,}\")\n",
        "print(f\"Nombre de cellules avec valeurs manquantes: {df.isnull().sum().sum():,}\")\n",
        "print(f\"Pourcentage global de valeurs manquantes: {(df.isnull().sum().sum() / df.size) * 100:.2f}%\")\n",
        "\n",
        "# Identification des variables problématiques (>10% de données manquantes)\n",
        "variables_problematiques = missing_df[missing_df['Pourcentage'] > 10]\n",
        "if len(variables_problematiques) > 0:\n",
        "    print(f\"\\n⚠ {len(variables_problematiques)} variable(s) avec >10% de données manquantes:\")\n",
        "    for idx, row in variables_problematiques.iterrows():\n",
        "        print(f\"  - {row['Variable']}: {row['Pourcentage']:.2f}%\")\n",
        "else:\n",
        "    print(\"\\n✓ Aucune variable avec plus de 10% de données manquantes\")\n",
        "\n",
        "# Analyse des lignes avec valeurs manquantes\n",
        "lignes_completes = df.dropna().shape[0]\n",
        "lignes_avec_manquantes = df.shape[0] - lignes_completes\n",
        "\n",
        "print(f\"\\nLignes complètes (sans valeurs manquantes): {lignes_completes} ({(lignes_completes/len(df))*100:.2f}%)者に)\")\n",
        "print(f\"Lignes avec au moins une valeur manquante: {lignes_avec_manquantes} ({(lignes_avec_manquantes/len(df))*100:.2f}%)者に)\")\n",
        "\n",
        "# Distribution du nombre de valeurs manquantes par ligne\n",
        "missing_per_row = df.isnull().sum(axis=1)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DISTRIBUTION DU NOMBRE DE VALEURS MANQUANTES PAR LIGNE\")\n",
        "print(\"-\"*80)\n",
        "print(missing_per_row.value_counts().sort_index())\n",
        "\n",
        "# Test de pattern de données manquantes (MCAR, MAR, MNAR)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"ANALYSE DES PATTERNS DE DONNÉES MANQUANTES\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Corrélation entre les données manquantes\n",
        "missing_matrix = df.isnull().astype(int)\n",
        "missing_corr = missing_matrix.corr()\n",
        "\n",
        "# Identifier les paires de variables avec forte corrélation de manquance\n",
        "high_corr_missing = []\n",
        "for i in range(len(missing_corr.columns)):\n",
        "    for j in range(i+1, len(missing_corr.columns)):\n",
        "        if abs(missing_corr.iloc[i, j]) > 0.3 and missing_corr.iloc[i, j] != 1:\n",
        "            high_corr_missing.append({\n",
        "                'Var1': missing_corr.columns[i],\n",
        "                'Var2': missing_corr.columns[j],\n",
        "                'Corrélation': missing_corr.iloc[i, j]\n",
        "            })\n",
        "\n",
        "if high_corr_missing:\n",
        "    print(\"\\n⚠ Corrélations significatives détectées entre les patterns de données manquantes:\")\n",
        "    print(\"   (Suggère que les données ne sont peut-être pas MCAR)\")\n",
        "    for item in high_corr_missing:\n",
        "        print(f\"  - {item['Var1']} ↔ {item['Var2']}: r = {item['Corrélation']:.3f}\")\n",
        "else:\n",
        "    print(\"\\n✓ Pas de corrélation forte entre les patterns de données manquantes\")\n",
        "    print(\"   (Compatible avec l'hypothèse MCAR - Missing Completely At Random)\")\n",
        "\n",
        "print(\"\\n✓ Analyse des valeurs manquantes terminée\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ANALYSE DES VALEURS MANQUANTES\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TABLEAU RÉCAPITULATIF DES VALEURS MANQUANTES\n",
            "--------------------------------------------------------------------------------\n",
            "Variable  Valeurs_manquantes  Pourcentage     Type\n",
            "     A10                  43         6.23 category\n",
            "      A5                  42         6.09 category\n",
            "      A3                  39         5.65  float64\n",
            "      A1                  39         5.65 category\n",
            "     A12                  39         5.65 category\n",
            "     A14                  38         5.51  float64\n",
            "      A7                  38         5.51 category\n",
            "      A2                  38         5.51  float64\n",
            "      A4                  37         5.36 category\n",
            "      A9                  37         5.36 category\n",
            "     A15                  33         4.78  float64\n",
            "      A6                  32         4.64 category\n",
            "     A13                  29         4.20 category\n",
            "      A8                  26         3.77  float64\n",
            "     A11                  22         3.19  float64\n",
            "     A16                   0         0.00 category\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STATISTIQUES GLOBALES\n",
            "--------------------------------------------------------------------------------\n",
            "Nombre total de cellules: 11,040\n",
            "Nombre de cellules avec valeurs manquantes: 532\n",
            "Pourcentage global de valeurs manquantes: 4.82%\n",
            "\n",
            "✓ Aucune variable avec plus de 10% de données manquantes\n",
            "\n",
            "Lignes complètes (sans valeurs manquantes): 315 (45.65%)者に)\n",
            "Lignes avec au moins une valeur manquante: 375 (54.35%)者に)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DISTRIBUTION DU NOMBRE DE VALEURS MANQUANTES PAR LIGNE\n",
            "--------------------------------------------------------------------------------\n",
            "0    315\n",
            "1    262\n",
            "2     78\n",
            "3     26\n",
            "4      9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ANALYSE DES PATTERNS DE DONNÉES MANQUANTES\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "✓ Pas de corrélation forte entre les patterns de données manquantes\n",
            "   (Compatible avec l'hypothèse MCAR - Missing Completely At Random)\n",
            "\n",
            "✓ Analyse des valeurs manquantes terminée\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0253817e"
      },
      "source": [
        "### Traitement des valeurs manquantes\n",
        "\n",
        "Ce bloc de code met en œuvre une stratégie d'imputation pour gérer les valeurs manquantes dans le jeu de données. Pour les variables continues, il utilise la **médiane** comme méthode d'imputation, car elle est plus robuste aux valeurs aberrantes que la moyenne. Pour les variables catégorielles, il utilise le **mode** (la valeur la plus fréquente) pour remplacer les valeurs manquantes. Une copie du DataFrame original est créée (`df_cleaned`) pour appliquer ces transformations, assurant ainsi que le DataFrame original reste inchangé pour d'éventuelles analyses ultérieures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599460d2",
        "outputId": "6280c05d-debd-44f4-b22c-68c51d150f2c"
      },
      "source": [
        "# ============================================================================\n",
        "# TRAITEMENT DES VALEURS MANQUANTES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAITEMENT DES VALEURS MANQUANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Création d'une copie du DataFrame pour le traitement\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STRATÉGIES DE TRAITEMENT PAR TYPE DE VARIABLE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# 1. TRAITEMENT DES VARIABLES CONTINUES\n",
        "print(\"\\n1. Variables continues - Imputation par la médiane\")\n",
        "print(\"   (Plus robuste aux outliers que la moyenne)\")\n",
        "\n",
        "for col in variables_continues:\n",
        "    missing_count = df_cleaned[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        # Calcul de la médiane avant imputation\n",
        "        mediane = df_cleaned[col].median()\n",
        "        # Imputation\n",
        "        df_cleaned[col].fillna(mediane, inplace=True)\n",
        "        print(f\"   ✓ {col}: {missing_count} valeurs imputées avec la médiane ({mediane:.2f})\")\n",
        "    else:\n",
        "        print(f\"   ✓ {col}: aucune valeur manquante\")\n",
        "\n",
        "# 2. TRAITEMENT DES VARIABLES CATÉGORIELLES\n",
        "print(\"\\n2. Variables catégorielles - Imputation par le mode\")\n",
        "print(\"   (Valeur la plus fréquente)\")\n",
        "\n",
        "for col in variables_categorielles:\n",
        "    missing_count = df_cleaned[col].isnull().sum()\n",
        "    if missing_count > 0:\n",
        "        # Calcul du mode avant imputation\n",
        "        mode_value = df_cleaned[col].mode()[0]\n",
        "        # Imputation\n",
        "        df_cleaned[col].fillna(mode_value, inplace=True)\n",
        "        print(f\"   ✓ {col}: {missing_count} valeurs imputées avec le mode ('{mode_value}')\")\n",
        "    else:\n",
        "        print(f\"   ✓ {col}: aucune valeur manquante\")\n",
        "\n",
        "# 3. VÉRIFICATION DE LA VARIABLE C"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAITEMENT DES VALEURS MANQUANTES\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STRATÉGIES DE TRAITEMENT PAR TYPE DE VARIABLE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Variables continues - Imputation par la médiane\n",
            "   (Plus robuste aux outliers que la moyenne)\n",
            "   ✓ A2: 38 valeurs imputées avec la médiane (44.84)\n",
            "   ✓ A3: 39 valeurs imputées avec la médiane (10.38)\n",
            "   ✓ A8: 26 valeurs imputées avec la médiane (7.04)\n",
            "   ✓ A11: 22 valeurs imputées avec la médiane (14.50)\n",
            "   ✓ A14: 38 valeurs imputées avec la médiane (520.16)\n",
            "   ✓ A15: 33 valeurs imputées avec la médiane (24722.00)\n",
            "\n",
            "2. Variables catégorielles - Imputation par le mode\n",
            "   (Valeur la plus fréquente)\n",
            "   ✓ A1: 39 valeurs imputées avec le mode ('b')\n",
            "   ✓ A4: 37 valeurs imputées avec le mode ('t')\n",
            "   ✓ A5: 42 valeurs imputées avec le mode ('g')\n",
            "   ✓ A6: 32 valeurs imputées avec le mode ('k')\n",
            "   ✓ A7: 38 valeurs imputées avec le mode ('bb')\n",
            "   ✓ A9: 37 valeurs imputées avec le mode ('t')\n",
            "   ✓ A10: 43 valeurs imputées avec le mode ('t')\n",
            "   ✓ A12: 39 valeurs imputées avec le mode ('t')\n",
            "   ✓ A13: 29 valeurs imputées avec le mode ('p')\n"
          ]
        }
      ]
    }
  ]
}